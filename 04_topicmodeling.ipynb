{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on web-pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the default topic-modeling notebook proposed by dataiku.\n",
    "\n",
    "Please be carefull: We are playing here with the whole dataset. Vectorizing, for instance, will take several hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" />\n",
    "\n",
    "**This notebook requires the installation of the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html#installation) package.**\n",
    "[See here for help with intalling python packages.](https://www.dataiku.com/learn/guide/code/python/install-python-packages.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# to install the required packages. Uncomment the two next lines if necessary\n",
    "# !pip install --upgrade pip\n",
    "# !pip install pandas seaborn sklearn pyldavis\n",
    "\n",
    "%pylab inline\n",
    "import warnings                         # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "#from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the French and English corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load web-pages.csv as a dataframe\n",
    "web_pages = pd.read_csv('data/web-pages.csv') # add nrows = 1000 to load only the first 1000 lines.\n",
    "\n",
    "# We create a mask where the language column = en\n",
    "web_pages_mask_en = web_pages['language']=='en'\n",
    "# web_pages_mask_fr = web_pages['language']=='fr'\n",
    "\n",
    "# We apply this mask to web_pages so that our new dataframe is web_pages_en\n",
    "web_pages_en = web_pages[web_pages_mask_en]\n",
    "# web_pages_fr = web_pages[web_pages_mask_fr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we delete the web_pages dataframe as we don't need it anymore\n",
    "del web_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawl_date</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>mime_type_web_server</th>\n",
       "      <th>mime_type_tika</th>\n",
       "      <th>language</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20120118</td>\n",
       "      <td>feedburner.com</td>\n",
       "      <td>http://feeds.feedburner.com/QuoteSnack</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Stop SOPA! If Congress passes SOPA, the Intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120118</td>\n",
       "      <td>thedeadcrows.com</td>\n",
       "      <td>http://thedeadcrows.com/sopa.html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>STOP SOPA! STOP SOPA! STOP SOPA! This site has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20120118</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>http://www.youtube.com/embed/WU_gZNVfuf0?wmode...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>text/html</td>\n",
       "      <td>en</td>\n",
       "      <td>Life in a Day Soundtrack - Angolan Women - You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20120118</td>\n",
       "      <td>selectivism.com</td>\n",
       "      <td>http://selectivism.com/2010/02/</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>2010  February : Selectivism ABOUT TIP-OFF CON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20120118</td>\n",
       "      <td>hackedgadgets.com</td>\n",
       "      <td>http://hackedgadgets.com/2007/02/</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>2007  February - Hacked Gadgets – DIY Tech Blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219986</th>\n",
       "      <td>20120118</td>\n",
       "      <td>eff.org</td>\n",
       "      <td>https://www.eff.org/deeplinks/2012/01/sites/al...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Electronic Frontier Foundation | Defending you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219987</th>\n",
       "      <td>20120118</td>\n",
       "      <td>eff.org</td>\n",
       "      <td>https://www.eff.org/deeplinks/2012/01/sites/al...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Electronic Frontier Foundation | Defending you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219988</th>\n",
       "      <td>20120118</td>\n",
       "      <td>eff.org</td>\n",
       "      <td>https://www.eff.org/deeplinks/2012/01/sites/al...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Electronic Frontier Foundation | Defending you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219989</th>\n",
       "      <td>20120118</td>\n",
       "      <td>eff.org</td>\n",
       "      <td>https://www.eff.org/deeplinks/2012/01/0.8</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Electronic Frontier Foundation | Defending you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219990</th>\n",
       "      <td>20120118</td>\n",
       "      <td>eff.org</td>\n",
       "      <td>https://www.eff.org/deeplinks/2012/01/form.new...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>application/xhtml+xml</td>\n",
       "      <td>en</td>\n",
       "      <td>Electronic Frontier Foundation | Defending you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135484 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        crawl_date             domain  \\\n",
       "1         20120118     feedburner.com   \n",
       "2         20120118   thedeadcrows.com   \n",
       "3         20120118        youtube.com   \n",
       "4         20120118    selectivism.com   \n",
       "6         20120118  hackedgadgets.com   \n",
       "...            ...                ...   \n",
       "219986    20120118            eff.org   \n",
       "219987    20120118            eff.org   \n",
       "219988    20120118            eff.org   \n",
       "219989    20120118            eff.org   \n",
       "219990    20120118            eff.org   \n",
       "\n",
       "                                                      url  \\\n",
       "1                  http://feeds.feedburner.com/QuoteSnack   \n",
       "2                       http://thedeadcrows.com/sopa.html   \n",
       "3       http://www.youtube.com/embed/WU_gZNVfuf0?wmode...   \n",
       "4                         http://selectivism.com/2010/02/   \n",
       "6                       http://hackedgadgets.com/2007/02/   \n",
       "...                                                   ...   \n",
       "219986  https://www.eff.org/deeplinks/2012/01/sites/al...   \n",
       "219987  https://www.eff.org/deeplinks/2012/01/sites/al...   \n",
       "219988  https://www.eff.org/deeplinks/2012/01/sites/al...   \n",
       "219989          https://www.eff.org/deeplinks/2012/01/0.8   \n",
       "219990  https://www.eff.org/deeplinks/2012/01/form.new...   \n",
       "\n",
       "       mime_type_web_server         mime_type_tika language  \\\n",
       "1                 text/html              text/html       en   \n",
       "2                 text/html  application/xhtml+xml       en   \n",
       "3                 text/html              text/html       en   \n",
       "4                 text/html  application/xhtml+xml       en   \n",
       "6                 text/html  application/xhtml+xml       en   \n",
       "...                     ...                    ...      ...   \n",
       "219986            text/html  application/xhtml+xml       en   \n",
       "219987            text/html  application/xhtml+xml       en   \n",
       "219988            text/html  application/xhtml+xml       en   \n",
       "219989            text/html  application/xhtml+xml       en   \n",
       "219990            text/html  application/xhtml+xml       en   \n",
       "\n",
       "                                                  content  \n",
       "1       Stop SOPA! If Congress passes SOPA, the Intern...  \n",
       "2       STOP SOPA! STOP SOPA! STOP SOPA! This site has...  \n",
       "3       Life in a Day Soundtrack - Angolan Women - You...  \n",
       "4       2010  February : Selectivism ABOUT TIP-OFF CON...  \n",
       "6       2007  February - Hacked Gadgets – DIY Tech Blo...  \n",
       "...                                                   ...  \n",
       "219986  Electronic Frontier Foundation | Defending you...  \n",
       "219987  Electronic Frontier Foundation | Defending you...  \n",
       "219988  Electronic Frontier Foundation | Defending you...  \n",
       "219989  Electronic Frontier Foundation | Defending you...  \n",
       "219990  Electronic Frontier Foundation | Defending you...  \n",
       "\n",
       "[135484 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We display our dataframe web_pages_en\n",
    "web_pages_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, some fun with the english corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crawl_date', 'domain', 'url', 'mime_type_web_server', 'mime_type_tika',\n",
       "       'language', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we display the columns so that we can choose the right one to process the topic modelling\n",
    "web_pages_en.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         Stop SOPA! If Congress passes SOPA, the Intern...\n",
       "2         STOP SOPA! STOP SOPA! STOP SOPA! This site has...\n",
       "3         Life in a Day Soundtrack - Angolan Women - You...\n",
       "4         2010  February : Selectivism ABOUT TIP-OFF CON...\n",
       "6         2007  February - Hacked Gadgets – DIY Tech Blo...\n",
       "                                ...                        \n",
       "219986    Electronic Frontier Foundation | Defending you...\n",
       "219987    Electronic Frontier Foundation | Defending you...\n",
       "219988    Electronic Frontier Foundation | Defending you...\n",
       "219989    Electronic Frontier Foundation | Defending you...\n",
       "219990    Electronic Frontier Foundation | Defending you...\n",
       "Name: content, Length: 135484, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define the column that we will choose for the topic modelling\n",
    "raw_text_col_en = \"content\"\n",
    "# We create the dataframe with the \"content\" column only\n",
    "raw_text_en = web_pages_en[raw_text_col_en]\n",
    "# We remove the NaN to be sure\n",
    "raw_text_en = raw_text_en.dropna()\n",
    "# We display the result (to be sure we did what we wanted to do)\n",
    "raw_text_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "\n",
    "We cannot directly feed the text to the Topics Extraction Algorithms. We first need to process the text in order to get numerical vectors. We achieve this by applying either a CountVectorizer() or a TfidfVectorizer(). For more information on those technics, please refer to thid [sklearn documentation](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any text mining task, we first need to remove stop words that provide no useful information about topics. *sklearn* provides a default stop words list for english, but we can alway add to it any custom stop words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove the stopwords\n",
    "\n",
    "# Uncomment the next line to define custom stopwords\n",
    "#custom_stop_words = [u'did', u'good', u'right', u'said', u'does', u'way',u'edu', u'com', u'mail', u'thanks', u'post', u'address', u'university', u'email', u'soon', u'article',u'people', u'god', u'don', u'think', u'just', u'like', u'know', u'time', u'believe', u'say',u'don', u'just', u'think', u'probably', u'use', u'like', u'look', u'stuff', u'really', u'make', u'isn']\n",
    "\n",
    "stop_words_en = text.ENGLISH_STOP_WORDS #.union(custom_stop_words) - add this part if you added custom stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a CountVectorizer() object and then apply the fit_transform method to the text.\n",
    "\n",
    "**This will take ages (several hours), be very patient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135484, 209533)\n"
     ]
    }
   ],
   "source": [
    "cnt_vectorizer_en = CountVectorizer(stop_words = stop_words_en,lowercase = True,\n",
    "                    token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.85, min_df = 2)\n",
    "\n",
    "text_cnt_en = cnt_vectorizer_en.fit_transform(raw_text_en)\n",
    "\n",
    "print(text_cnt_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer() on the text data <a id=\"tfidf\" /> \n",
    "\n",
    "We first initialise a TfidfVectorizer() object and then apply the fit_transform method to the text.\n",
    "\n",
    "**Same: ages, patience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135484, 1114)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_en = TfidfVectorizer(strip_accents = 'unicode',stop_words = stop_words_en,lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b', max_df = 0.75, min_df = 0.02)\n",
    "\n",
    "text_tfidf_en = tfidf_vectorizer_en.fit_transform(raw_text_en)\n",
    "\n",
    "print(text_tfidf_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will apply the topics extraction to `text_tidf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topics Extraction Models <a id=\"mod\" /> \n",
    "\n",
    "There are two very popular models for topic modelling, both available in the sklearn library: \n",
    "\n",
    "* [NMF (Non-negative Matrix Factorization)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization),\n",
    "* [LDA (Latent Dirichlet Allocation)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "Those two topic modeling algorithms infer topics from a collection of texts by viewing each document as a mixture of various topics. The only parameter we need to choose is the number of desired topics `n_topics`.  \n",
    "It is recommended to try different values for `n_topics` in order to find the most insightful topics. For that, we will show below different analyses (most frequent words per topics and heatmaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this line for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_en = LatentDirichletAllocation(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_model_en.fit(text_tfidf_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NMF instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_model_nmf_en = NMF(n_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics_model_nmf.fit(text_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Frequent Words per Topics\n",
    "An important way to assess the validity of our topic modelling is to directly look at the most frequent words in each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederic.clavert/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/90/j99p488148vcmjxgs16_mq0j42556r/T/ipykernel_83579/364927228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature_names_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_top_words\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_model_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic #%d:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtopic_idx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mget_top_words_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "n_top_words = 25\n",
    "feature_names_en = tfidf_vectorizer_en.get_feature_names()\n",
    "\n",
    "def get_top_words_topic(topic_idx):\n",
    "    topic = topics_model_en.components_[topic_idx]\n",
    "   \n",
    "    print( [feature_names_en[i] for i in topic.argsort()[:-n_top_words - 1:-1]] )\n",
    "    \n",
    "for topic_idx, topic in enumerate(topics_model_en.components_):\n",
    "    print (\"Topic #%d:\" % topic_idx )\n",
    "    get_top_words_topic(topic_idx)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the words present, if some are very common you may want to go back to the [definition of custom stop words](#stop_words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Naming the topics\n",
    "\n",
    "Thanks to the above analysis, we can try to name each topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic_name = {i: \"topic_\"+str(i) for i in range(n_topics)}\n",
    "#dict_topic_name = my_dict_topic_name #Define here your own name mapping and uncomment this !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Heatmaps\n",
    "\n",
    "Another visual helper to better understand the found topics is to look at the heatmap for the document-topic and topic-words matrices. This gives us the distribution of topics over the collection of documents and the distribution of words over the topics.  \n",
    "We start with the topic-word heatmap where the darker the color is the more the word is representative of the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = pd.DataFrame(topics_model_en.components_.T)\n",
    "word_model.index = feature_names_en\n",
    "word_model.columns.name = 'topic'\n",
    "word_model['norm'] = (word_model).apply(lambda x: x.abs().max(),axis=1)\n",
    "word_model = word_model.sort_values(by='norm',ascending=0) # sort the matrix by the norm of row vector\n",
    "word_model.rename(columns = dict_topic_name, inplace = True) #naming topic\n",
    " \n",
    "del word_model['norm']\n",
    "\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.heatmap(word_model[:25]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the document-topic heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the document-topic matrix\n",
    "document_model = pd.DataFrame(topics_model_en.transform(text_tfidf_en))\n",
    "document_model.columns.name = 'topic'\n",
    "document_model.rename(columns = dict_topic_name, inplace = True) #naming topics\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(document_model.sort_index()[:100]) #we limit here to the first 10 texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic distribution over the corpus  \n",
    "We can look at how the topics are represented in the collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_proportion = document_model.sum()/document_model.sum().sum()\n",
    "topics_proportion.plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we can investigate the documents the most representative for the given topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_documents_topics(topic_name, n_doc = 3, excerpt = True):\n",
    "    '''This returns the n_doc documents most representative of topic_name'''\n",
    "    \n",
    "    document_index = list(document_model[topic_name].sort_values(ascending = False).index)[:n_doc]\n",
    "    for order, i in enumerate(document_index):\n",
    "        print (\"Text for the {}-th most representative document for topic {}:\\n\".format(order + 1,topic_name))\n",
    "        if excerpt:\n",
    "            raw_text_en[i][:100]\n",
    "        else:\n",
    "            raw_text_en[i]\n",
    "        \"\\n******\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_documents_topics(\"topic_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Visualization with pyLDAvis <a id=\"viz\">\n",
    "\n",
    "Thanks to the pyLDAvis package, we can easily visualise and interpret the topics that has been fit to our corpus of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimvis.prepare(topics_model_en, text_tfidf_en, tfidf_vectorizer_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Clustering  <a id=\"clust\">  \n",
    "\n",
    "Once we have fitted topics on the text data, we can try to understand how they relate to one another: we achieve this by doing a hierachical clustering on the topics. We propose two methods, the first is based on a correlation table between topics, the second on a contigency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix between topics\n",
    "cor_matrix = np.corrcoef(document_model.iloc[:,:n_topics].values,rowvar=0)\n",
    "\n",
    "#Renaming of the index and columns\n",
    "cor_matrix = pd.DataFrame(cor_matrix)\n",
    "cor_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "cor_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(cor_matrix, cmap=\"bone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contingency table on the binarized document-topic matrix\n",
    "document_bin_topic = (document_model.iloc[:,:n_topics] > 0.25).astype(int)\n",
    "contingency_matrix = np.dot(document_bin_topic.T.values, document_bin_topic.values )\n",
    "\n",
    "#Renaming of the index and columns\n",
    "contingency_matrix = pd.DataFrame(contingency_matrix)\n",
    "contingency_matrix.rename(index = dict_topic_name, inplace = True)\n",
    "contingency_matrix.rename(columns= dict_topic_name, inplace = True)\n",
    "\n",
    "sns.clustermap(contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further steps  <a id=\"next\">  \n",
    "\n",
    "Topics extraction is a vast subject and a notebook can only show so much. There still much thing we could do, here are some ideas:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard documents from noise topics\n",
    "The following helper function takes as argument the topics for which we wish to discard documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doc(*topic_name):\n",
    "    \n",
    "    doc_max_topic = document_model.idxmax(axis = 1)\n",
    "    \"Removing documents whose main topic are in \", topic_name\n",
    "    doc_max_topic_filtered = doc_max_topic[~doc_max_topic.isin(topic_name)]\n",
    "    return [raw_text[i] for i in doc_max_topic_filtered.index.tolist()]\n",
    "\n",
    "#E.g.: to remove documents whose main topic are topic_1 or topic_3, we would simply call remove_doc(\"topic_0\",\"topic_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 20newsgroup dataset, try this to remove text of topic \"Misc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_text_filtered = remove_doc(\"Misc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring the topic model on new text\n",
    "Finally, we can score new text with our topic model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = raw_text_en[:3] #Change this to the new text you'd like to score !\n",
    "\n",
    "tfidf_new_text_en = tfidf_vectorizer_en.transform(new_text)\n",
    "result = pd.DataFrame(topics_model_en.transform(tfidf_new_text_en), columns = [dict_topic_name[i] for i in range(n_topics)])\n",
    "sns.heatmap(result)"
   ]
  }
 ],
 "metadata": {
  "analyzedDataset": "AWAC2_webpage_FR",
  "createdOn": 1634445698186,
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "modifiedBy": "admin",
  "tags": [],
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
